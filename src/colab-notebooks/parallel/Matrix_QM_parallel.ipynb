{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> ### Matrix_QM_parallel.ipynb\n",
        "> ### Parallel Lanczos Algorithm for 1D Quantum Mechanics\n",
        "> ### Hybrid MPI + OpenMP implementation with domain decomposition\n",
        "> ### Author: Dhruv Patel (2130292), Mohammadreza Khansari (2132180)\n",
        "> ### Date: 31st March 2025"
      ],
      "metadata": {
        "id": "P78jJhpRCp6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQRo64KnaDD7",
        "outputId": "0d39661b-39fc-478c-dc8d-79fe30544229"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/466.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4458265 sha256=1dd0efe32364c1799064bae4bf20696b8b2bf02dc584fb7ce40eceaa50aad3b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__A6ZZXJZ5Zx",
        "outputId": "3ef0443a-87e6-402a-9f53-fbbe8551e9fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max orthogonality error after step 11: 1.94e-15\n",
            "Max orthogonality error after step 21: 3.62e-15\n",
            "Max orthogonality error after step 31: 4.58e-15\n",
            "Max orthogonality error after step 41: 6.36e-15\n",
            "Max orthogonality error after step 51: 9.31e-15\n",
            "Max orthogonality error after step 61: 1.27e-14\n",
            "Max orthogonality error after step 71: 1.50e-14\n",
            "Max orthogonality error after step 81: 1.77e-14\n",
            "Iteration time: 6.4108s (excluding tridiagonal solve)\n",
            "\n",
            "-----------------------------------------\n",
            "\n",
            "Computed eigenvalues: [  3.52792068  14.00078376  31.74586727  56.57273612  83.56219275\n",
            " 126.33345481 177.60271357 232.9663294  298.62896338 361.52874642]\n",
            "\n",
            "Execution time: 6.4167 seconds\n",
            "\n",
            "Relative error: [ 6.05584137  8.33385584 11.69834691 15.16363889 17.56937617 21.96971906\n",
            " 26.3234944  30.06217725 34.13281922 37.05565752]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from mpi4py import MPI\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "# =============================================================================\n",
        "# Lanczos Algorithm Implementation (Parallel Version)\n",
        "# =============================================================================\n",
        "\n",
        "def Lanczos_algorithm(m, n, a, rank, num_procs, comm):\n",
        "    \"\"\"\n",
        "    Implements the parallel Lanczos algorithm using MPI domain decomposition\n",
        "\n",
        "    Each MPI process works on a local segment of the full wavefunction\n",
        "    Communication is required to exchange boundary values between processes\n",
        "\n",
        "    Args:\n",
        "        m (int): Krylov subspace size (iteration)\n",
        "        n (int): Total number of grid points\n",
        "        a (float): Grid spacing\n",
        "        rank (int): MPI rank of the current process\n",
        "        num_procs (int): Total number of MPI processes\n",
        "        comm (MPI.Comm): MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        eigenvalues (ndarray): 10 smallest eigenvalues (root process only)\n",
        "    \"\"\"\n",
        "\n",
        "    # Domain decomposition: Splitting grid points across processes\n",
        "    sizes = distribute_elements(n, num_procs)\n",
        "    local_n = sizes[rank]\n",
        "    starts = [sum(sizes[:i]) for i in range(num_procs)]\n",
        "    start_idx = starts[rank]\n",
        "\n",
        "    # Precomputing the local position array for current process\n",
        "    M = (n - 1) // 2\n",
        "    x_local = a * np.arange(start_idx - M, start_idx - M + local_n)\n",
        "\n",
        "    # Initializing Lanczos vectors and coefficients\n",
        "    v = np.zeros((local_n, m))    # Orthonormal basis\n",
        "    B = np.zeros(m-1)             # Off-diagonal elements\n",
        "    alpha = np.zeros(m)           # Diagonal elements\n",
        "\n",
        "    # Initializing the starting vector (local part) and normalizing it globally\n",
        "    v1 = np.random.randn(local_n)\n",
        "    v1 = normalize_vector(v1, comm)\n",
        "\n",
        "    # First Lanczos iteration: applying Ĥ using boundary exchange\n",
        "    v_temp = exchange_boundary_data(v1, local_n, rank, num_procs, comm)\n",
        "    w1_p = Hv(v_temp, a, x_local, comm)  # Now returns local_n elements\n",
        "    a1 = dot_product(w1_p, v1, comm)\n",
        "    w1 = w1_p - a1 * v1\n",
        "    alpha[0] = a1\n",
        "    v[:, 0] = v1\n",
        "\n",
        "    comm.Barrier()\n",
        "    start_time = MPI.Wtime()  # Start timing after initialization\n",
        "\n",
        "    # Lanczos iteration loop\n",
        "    for j in range(1, m):\n",
        "        # Computing the beta coefficient with global norm\n",
        "        B[j - 1] = get_global_norm(w1, comm)\n",
        "\n",
        "        # Handling the Lanczos breakdown\n",
        "        if np.isclose(B[j-1], 0.0):\n",
        "            # Restart if numerical breakdown occurs\n",
        "            v[:, j] = normalize_vector(np.random.randn(local_n), comm)\n",
        "        else:\n",
        "            v[:, j] = w1 / B[j-1]\n",
        "\n",
        "        # Function call: Modified Gram-Schmidt reorthogonalization\n",
        "        v = modified_gram_schmidt(v, j + 1, comm)\n",
        "\n",
        "        # Check orthogonality after every 10 steps\n",
        "        if j % 10 == 0:\n",
        "            check_orthogonality(v, j + 1, comm)\n",
        "\n",
        "        # Applying the Hamiltonian operator (with boundary communication)\n",
        "        v_temp = exchange_boundary_data(v[:, j], local_n, rank, num_procs, comm)\n",
        "        w1_p = Hv(v_temp, a, x_local, comm)\n",
        "        alpha[j] = dot_product(w1_p, v[:, j], comm)\n",
        "        w1 = w1_p - alpha[j]*v[:, j] - B[j - 1]*v[:, j - 1]\n",
        "\n",
        "    comm.Barrier()\n",
        "    iteration_time = MPI.Wtime() - start_time\n",
        "\n",
        "    # Root process (rank 0) solves tridiagonal system and evaluate eigenvalues\n",
        "    if rank == 0:\n",
        "        T = np.diag(alpha) + np.diag(B, -1) + np.diag(B, 1)\n",
        "        eigenvalues, _ = eigsh(T, k=10, which='SM')\n",
        "        print(f\"Iteration time: {iteration_time:.4f}s (excluding tridiagonal solve)\")\n",
        "    else:\n",
        "        eigenvalues = None\n",
        "\n",
        "    return eigenvalues\n",
        "\n",
        "# =============================================================================\n",
        "# MPI Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "def distribute_elements(N, num_procs):\n",
        "    \"\"\"\n",
        "    Distributes grid points evenly across MPI processes\n",
        "\n",
        "    Returns:\n",
        "        sizes (list): Number of points per process\n",
        "    \"\"\"\n",
        "    base, rem = divmod(N, num_procs)\n",
        "    return [base + 1 if i < rem else base for i in range(num_procs)]\n",
        "\n",
        "def exchange_boundary_data(v, local_n, rank, size, comm):\n",
        "    \"\"\"\n",
        "    Performs non-blocking communication to exchange ghost cell data with neighboring processes\n",
        "\n",
        "    This is needed because the Hamiltonian operator (finite differences) accesses neighboring elements\n",
        "\n",
        "    Args:\n",
        "        v (ndarray): Local vector segment\n",
        "        local_n (int): Local dimension\n",
        "        rank (int): Current process rank\n",
        "        size (int): Total number of processes\n",
        "        comm (MPI.Comm): MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Vector with ghost cells from neighbors\n",
        "    \"\"\"\n",
        "    right_rank = (rank + 1) % size\n",
        "    left_rank = (rank - 1) % size\n",
        "\n",
        "    # Preparing the send/receive buffers\n",
        "    send_buf = np.array([v[0], v[-1]], dtype=np.float64)\n",
        "    recv_buf = np.empty(2, dtype=np.float64)\n",
        "\n",
        "    # Non-blocking communications\n",
        "    reqs = [\n",
        "        comm.Isend(send_buf[0:1], dest=left_rank),\n",
        "        comm.Isend(send_buf[1:2], dest=right_rank),\n",
        "        comm.Irecv(recv_buf[0:1], source=right_rank),\n",
        "        comm.Irecv(recv_buf[1:2], source=left_rank)\n",
        "    ]\n",
        "    MPI.Request.Waitall(reqs)\n",
        "\n",
        "    # Building the extended local vector with ghost cells at boundaries\n",
        "    v_ext = np.empty(local_n + 2, dtype=np.float64)\n",
        "    v_ext[1:-1] = v         # Local data\n",
        "    v_ext[0] = recv_buf[1]  # From left neighbor's right\n",
        "    v_ext[-1] = recv_buf[0] # From right neighbor's left\n",
        "    return v_ext\n",
        "\n",
        "def check_orthogonality(v, k, comm):\n",
        "    \"\"\"Check orthogonality of vectors up to index `k`\n",
        "\n",
        "    Args:\n",
        "        v (ndarray): Matrix of vectors\n",
        "        k (int): Index up to which to check orthogonality\n",
        "        comm (MPI.Comm): MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ortho_errors = []\n",
        "    for i in range(k):\n",
        "        for j in range(i, k):\n",
        "\n",
        "            # Compute global dot product (parallel)\n",
        "            dot = dot_product(v[:, i], v[:, j], comm)\n",
        "            if i == j:\n",
        "                error = abs(dot - 1.0)  # Should be 1\n",
        "            else:\n",
        "                error = abs(dot)        # Should be 0\n",
        "            ortho_errors.append((i, j, error))\n",
        "\n",
        "    # Print max error from rank 0\n",
        "    if comm.rank == 0:\n",
        "        max_error = max([e for (_, _, e) in ortho_errors])\n",
        "        print(f\"Max orthogonality error after step {k}: {max_error:.2e}\")\n",
        "        if max_error > 1e-10:\n",
        "            print(\"WARNING: Orthogonality lost!\")\n",
        "\n",
        "def normalize_vector(v, comm):\n",
        "    \"\"\"Normalizes the vector using a global norm computed via MPI\"\"\"\n",
        "    return v / get_global_norm(v, comm)\n",
        "\n",
        "def get_global_norm(v, comm):\n",
        "    \"\"\"Computes the L2 norm of a vector across all MPI processes\"\"\"\n",
        "    local_sq = np.sum(v**2)\n",
        "    global_sq = comm.allreduce(local_sq, op=MPI.SUM)\n",
        "    return np.sqrt(global_sq)\n",
        "\n",
        "def dot_product(v, u, comm):\n",
        "    \"\"\"Computes the global dot product of two vectors across MPI processes\"\"\"\n",
        "    local_dot = v @ u\n",
        "    return comm.allreduce(local_dot, op=MPI.SUM)\n",
        "\n",
        "def Hv(v, a, x_local, comm):\n",
        "    \"\"\"Vectorized Hamiltonian application\"\"\"\n",
        "    \"\"\"\n",
        "    Applies the Vectorized Hamiltonian operator to a local segment of the\n",
        "    wavefunction to both kinetic and potential parts\n",
        "\n",
        "    Args:\n",
        "        v (ndarray): Local segment of ψ with ghost cells\n",
        "        a (float): Grid spacing\n",
        "        x_local (ndarray): Local position array\n",
        "        comm (MPI.Comm): MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        result (ndarray): Local segment of Hψ\n",
        "    \"\"\"\n",
        "    hbar = 1.0  # Assuming natural units\n",
        "\n",
        "    # Kinetic energy (sparse momentum operator)\n",
        "    kinetic = (hbar**2 / (2*a**2)) * (2*v[1:-1] - v[:-2] - v[2:])\n",
        "\n",
        "    # Potential energy\n",
        "    potential = 0.5 * np.abs(x_local)**2 * v[1:-1]\n",
        "    return kinetic + potential  # Returns local_n elements\n",
        "\n",
        "def modified_gram_schmidt(matrix, k, comm):\n",
        "    \"\"\"\n",
        "    Applies the Parallel modified Modified Gram-Schmidt orthogonalization\n",
        "    Reorthogonalizes the current Lanczos vectors for better numerical stability\n",
        "\n",
        "    Args:\n",
        "        matrix (ndarray): Column vectors to orthogonalize\n",
        "        k (int): Number of vectors to process\n",
        "        comm (MPI.Comm): MPI communicator\n",
        "\n",
        "    Returns:\n",
        "        ndarray: Orthonormalized vectors\n",
        "    \"\"\"\n",
        "    for i in range(k):\n",
        "        # Normalizing the current vector\n",
        "        orthogonalized = matrix.copy()\n",
        "        norm = get_global_norm(orthogonalized[:, i], comm)\n",
        "        orthogonalized[:, i] /= norm\n",
        "\n",
        "        # Orthogonalizing the subsequent vectors\n",
        "        for j in range(i + 1, k):\n",
        "            proj = dot_product(orthogonalized[:, j], orthogonalized[:, i], comm)\n",
        "            orthogonalized[:, j] -= proj * orthogonalized[:, i]\n",
        "    return orthogonalized\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # MPI initialization\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    # Physical parameters\n",
        "    L = 25.0    # Physical system size\n",
        "    N = 2049    # Number of spatial grid points (2^16): ensuring 'odd' for symmetry\n",
        "    m = 90      # Krylov subspace size: controls Lanczos iterations\n",
        "    a = L / N   # Grid spacing\n",
        "\n",
        "    # Synchronize before timing\n",
        "    comm.Barrier()\n",
        "    start_time = MPI.Wtime()\n",
        "\n",
        "    # Executing the algorithm to evaluate eigenvalues\n",
        "    eigenvalues = Lanczos_algorithm(m, N, a, rank, size, comm)\n",
        "\n",
        "    end_time = MPI.Wtime()\n",
        "\n",
        "    # Saving the results (root process only)\n",
        "    if rank == 0:\n",
        "\n",
        "        # Storing the computed eigenvalues in text file\n",
        "        print(\"\\n-----------------------------------------\\n\")\n",
        "        print(f\"Computed eigenvalues: {eigenvalues}\")\n",
        "        exec_time = end_time - start_time\n",
        "        print(f\"\\nExecution time: {exec_time:.4f} seconds\")\n",
        "        np.savetxt('eigenvalues_parallel.txt', eigenvalues,\n",
        "                   header=f\"Eigenvalues computed with N = {N}, L = {L}, a = {a:.4f} (parallel execution)\\nExecution time: {exec_time:.4f} seconds\")\n",
        "\n",
        "        # Storing the computed Relative error in text file\n",
        "        # Expected eigenvalues: E_n = ħω(n + 1/2)\n",
        "        # For our parameters ω = sqrt(k/m) = 1 (natural units)\n",
        "        expected = [0.5 + i for i in range(10)]\n",
        "        relative_error = (eigenvalues - expected) / expected\n",
        "        print(f\"\\nRelative error:\", relative_error)\n",
        "        np.savetxt('relative_error_parallel.txt', relative_error,\n",
        "                   header=\"Relative error computed from eigenvalues and expected values (parallel execution)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "print(cores)"
      ],
      "metadata": {
        "id": "g2XqYn4ofdoc",
        "outputId": "1c0904f4-bc1e-4ef3-bf5c-19f906395344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default Nodes and CPUs per Runtime Type\n",
        "\n",
        "------------------------------\n",
        "\n",
        "1. ***CPU Runtime:***\n",
        "\n",
        "Nodes: 1 (single VM instance).\n",
        "\n",
        "CPUs: Typically 2 physical cores (4 logical threads via hyper-threading). Use os.cpu_count() to check (e.g., returns 2 for physical cores or 4 for logical threads).\n",
        "\n",
        "------------------------------\n",
        "2. ***T4 GPU Runtime:***\n",
        "\n",
        "Nodes: 1 (same VM as CPU runtime).\n",
        "\n",
        "CPUs: Same as CPU runtime (2 physical cores).\n",
        "\n",
        "GPU: 1 NVIDIA T4 GPU.\n",
        "\n",
        "------------------------------\n",
        "3. ***v2-8 TPU Runtime:***\n",
        "\n",
        "Nodes: 1 (single TPU node).\n",
        "\n",
        "CPUs: Same as CPU runtime (2 physical cores on the VM).\n",
        "\n",
        "TPU Cores: 8 TPU cores (part of the TPU accelerator, not CPU cores).\n",
        "\n",
        "------------------------------"
      ],
      "metadata": {
        "id": "El8dOEZaIedM"
      }
    }
  ]
}